package com.hadooptest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.BasicConfigurator;
import org.apache.hadoop.io.IOUtils;

import java.net.URI;
import java.io.FileOutputStream;
import java.io.FileInputStream;
import java.io.InputStream;
import java.io.OutputStream;

public class Download {
    public static void main( String[] args ) throws Exception {
        BasicConfigurator.configure();
        Configuration conf = new Configuration();
        //配置NameNode的地址
        URI uri = new URI("hdfs://192.168.20.130:8020");
        //指定用户名，获取FileSystem对象
        FileSystem fs = FileSystem.get(uri,conf,"hadoop");

        //方法一:采用FileSystem类自带的copyFromLocalFile接口下载文件
        //HDFS file
        Path src = new Path("/17034460218/input5_1/data.txt");
        //本地文件
        Path dst = new Path("F:\\javaEE\\hadoop\\src\\test.txt");

        //Linux下的ideal执行的操作
        //fs.copyToLocalFile(src,dst);

        //windows下执行的操作
        fs.copyToLocalFile(false,src,dst,true);

        /*方法二：采用流拷贝的方式下载文件
        //打开一个输入流<----hdfs
        InputStream is = fs.open("/mydir/test.txt");
        //构造一个输出流-----> F:\javaEE\hadoop\src\test.txt
        OutputStream os = new FileOutputStream("F:\\javaEE\\hadoop\\src\\test.txt");
        //使用工具类实现复制
        IOUtils.copyBytes(is,os,1024);
        is.close();
        os.close();
        */

        fs.close();
        System.out.println("下载文件成功!");
    }
}
